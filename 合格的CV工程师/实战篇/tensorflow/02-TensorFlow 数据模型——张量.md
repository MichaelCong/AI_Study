# TensorFlow 数据模型——张量

## 张量的概念

在TensorFlow程序中，所有的数据都是通过张量的形式来表示。从功能的角度上看，张量可以被简单理解为多维数组。其中0阶张量表示标量，也就是一个数；第一阶张量为向量，也就是一个一维数组；第n阶张量可以理解为一个n维数组。但**张量在TensorFlow中的实现并不是直接采用数组的形式**，它只是对**TensorFlow中运算结果的引用**。在张量中并没有真正保存数字，它保存的是如何得到这些数字的计算过程。

```
tensor_0 = 1                                          # 视为 0阶张量
tensor_1 = [b"Tensor",b"flow",b"is",b"great"]         # 视为 1阶张量
tensor_2 = [[False,True,False],
			[True,True,False]]						  # 视为 2阶张量
tensor_3 = [[[0,0,0],[0,0,1]],
			[[1,0,0],[1,0,1]],
			[[2,0,0],[2,0,1]],]						  # 视为 3阶张量
```

在上述示例中，1阶张量中字符串前的字符“b”的含义。在python中，字符串是一类很特殊的数据，字符串前面加一个字母，以明确告知编译器，要以什么样的处理方式来处理它。通常，在字符串前面有三类字符前缀，它们分别是“b/B”，“u/U”和“r/R”。

| 前缀 | 处理方式                                       |
| ---- | ---------------------------------------------- |
| b/B  | 表示引号中的若干字符都是字节数组，而非字符串。 |
| u/U  | 表示字符串是Unicode编码。                      |
| r/R  | 表示非转义的原始字符串。                       |

```
import tensorflow as tf
# tf.constant是一个计算，这个计算的结果为一个张量，保存在变量a中。
a = tf.constant([1.0,2.0],name="a")
b = tf.constant([2.0,3.0],name="b")
result = tf.add(a,b,name="add")
print(result)
​```
输出：
Tensor("add_2:0", shape=(2,), dtype=float32)
​```
```

从以上代码可以看出TensorFlow中的张量和NumPy中的数组不同，TensorFlow计算的结果不是一个具体的数字，而是一个张量的结构。从上面的代码的运行结果可以看出，一个张量中主要保存了三个属性：名字、维度和类型。

## 张量的使用

和TensorFlow的计算模型相比，TensorFlow的数据模型相对比较简单。张量使用主要可以总结为两大类。

- [ ] 第一类用途，对中间计算结果的引用

当一个计算包含很多中间结果时，使用张量可以大大提高代码的可读性。以下为使用张量和不使用张量记录中间结果来完成向量相加的功能的代码对比。

```
# 使用张量记录中间结果
a = tf.constant([1.0,2.0],name="a")
b = tf.constant([2.0,3.0],name="b")
result = a+b

# 直接计算向量的和，这样可读性会比较差
result = tf.constant(([1.0,2.0],name="a") + 
         tf.constant(([2.0,3.0],name="b") 
```

从上面的例子可以看到，a和b其实就是对常量生成这个运算结果的引用，这样子在做加法时可以直接使用这两个变量，而不需要再去生成这些常量。当计算的复杂度增加时（比如在构建深层神经网络时）通过张量来引用计算的中间结果可以使代码的可阅读性大大提升。同时通过张量来存储中间结果可以方便获取中间结果。比如在卷积神经网络中，卷积层或者池化层有可能改变张量的维度，通过result.get_shape函数来获取结果张量的维度信息可以免去人工计算的麻烦。

- [ ] 第二类用途，当计算图构造完成之后，张量可以用来获得计算结果

虽然张量本身没有存储具体的数字，但是通过TensorFlow的会话，就可以得到这些具体的数字。

![点关注不迷路，我们一起上高速](../image/AI_study.jpg)