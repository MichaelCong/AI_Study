# 损失函数定义

神经网络模型的效果以及优化的目标是通过损失函数（loss function）来定义的 。

## 经典损失函数

分类问题和回归问题是监督学习的两大种类。

分类问题希望解决的是将不同的样本分到事先定义好的类别中。判断一个零件是否合格的问题就是一个二 分类问题。

交叉熵：刻画了两个概率分布之间的距离 , 它是分类问题中使用比较广的一种损失函数。

交叉熵是一个信息论中的概念,它原本是用来估算平均编码长度的。给定两个概率分布 p 和 q , 通过 q 来表示 p 的交叉煽为 :
$$
H(p,q)=-\sum_{x}p(x)log q(x)
$$
注意交叉熵：刻画的是两个概率分布之间的距离 , 然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率。当事件总数有限的情况下 ,概率分布函数 p(X = x) 满足 :
$$
\forall x  \quad p(X=x)\epsilon[0,1]且\sum_{x}p(X=x)=1
$$
也就是说，任意事件发生的概率都在 0 和 1 之间，且总有某 一个事件发生 ( 概率的和为 1 )。

对于多分类问题，Softmax是一个非常常用的方法。Softmax 回归本身可以作为一个学习算法来优化分类结果，但在 TensorFlow中， Softmax回归的参数被去掉了，它只是一 层额外的处理层,将神经网络的输出变成一个概率分布。下图展示了加上了 Softmax 回归的神经网络结构图 。

![](../image/softmax.png)

假设原始的神经网络输出为 y1,y2...,那么经过 Softmax 回归处理之后的输出为 :
$$
softmax(y)_i=y_j'=\frac{e^{yj}}{\sum_{j=1}^{n}{e^{yj}}}
$$
从以上公式中可以看出，原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多大。这样就把神经网络的输出也变成了 一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。

从交叉熵的公式中可以看到交叉熵不是对称的 （H(p, q)不等于H(q,p )） ,它刻画的是通过概率分布 q 来表达概率分布 p 的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时， p 代表的是正确答案， q 代表的是预测值。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵值越小，两个概率分布越接近。

## 自定义损失函数

在预测商品销量时，如果预测多了（预测值比真实销量大），商家损失的是生产商品的成本；而如果预测少了(预测值比真实销量小），损失的则是商品的利润。因为 一般商品的成本和商品的利润不会严格相 等，,所以使用均方误差损失函数就不能够很好地最大化销售利润 。 比如如果 一个商品的成本是1 元,但是利润是 10 元，,那么少预测一个就少挣 10 元；而多预测一个才少挣 1 元 。如果神经网络模型最小化的是均方误差，那么很有可能此模型就无法最大化预期的利润 。 为了最大化预期利润,需要将损失函数和利润直接联系起来 。注意损失函数定义的是损失，所以要将利润最大化,定义的损失函数应该刻画成本或者代价 。 以下公式给出了一个当预测多于真实值和预测少于真实值时有不同损失系数的损失函数:
$$
Loss(y,y')=\sum_{i=1}^{n}f(y_i,y_i'),\quad f(x,y)=\begin{cases} a(x-y)\quad x>y\\ b(y-x) \quad x \leq y \end{cases}
$$
TensorFlow中实现这个损失函数：

```
loss =tf.reduce_sum(tf.where(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))
```

以上代码用到了 tf.greater 和 tf.where 来实现选择操作 。 tf.greater 的输入是两个张量 ，此函数会比较这两个输入张量 中每 一个元素的大小，并返回比较结果 。

当 tf.greater 的输入张量维度不一样时, TensorFlow 会进行类似 NumPy 广播操作( broadcasting )的处理。tf.where函数有三个参数 。 第 一个为选择条件根据，当选择条件为 True 时，tf.where 函数会选择第二个参数中的值，否则使用第三个参数中的值。

```
import tensorflow as tf
from numpy.random import RandomState
batch_size = 8
# 两个输入节点
x = tf.placeholder(tf.float32,shape=(None,2),name='x-input')
# 回归问题一般只有一个输出节点
y_ = tf.placeholder(tf.float32,shape=(None,1),name='y-input')
# 定义了一个单层的神经网络前向传播的过程，这里就是简单加权和
w1 = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))
y = tf.matmul(x,w1)
# 定义预测多了和预测少了的成本
loss_less = 10
loss_more = 1
loss = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*loss_more,(y_-y)*loss_less))
train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)
#　通过随机数生成一个模拟数据集
rdm = RandomState(1)
dataset_size = 128
X = rdm.rand(dataset_size,2)
# 设置回归的正确值为两个输入的和加上一个随机量。之所以要加上一个随机量是为了
# 加入不可预测的噪声，否则不同损失函数的一样就不大了，因为不同损失函数都会在能
#完全预测正确的时候最低。一般来说噪声为了一个均值为０的小量，所以这里噪声设置为
#－-0.05~0.05的随机数
Y= [[x1+x2+rdm.rand()/10.0-0.05] for (x1,x2) in X]
# 训练神经网络
with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    STEPS = 5000
    for i in range(STEPS):
        start = (i*batch_size) % dataset_size
        end = min(start+batch_size,dataset_size)
        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})
        print(sess.run(w1))
```

运行以上代码会得到w1的值不是x1+x2，因为在损失函数中指定预测少了的损失更大（loss_less>loss_more）。